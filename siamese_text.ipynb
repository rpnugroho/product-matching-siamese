{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese-text.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj0yVWjh24NA"
      },
      "source": [
        "# !unzip /content/drive/Shareddrives/DATASET/ndsc-product-matching.zip -d /content/data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUtRrHIa5hZj"
      },
      "source": [
        "!cp -r '/content/drive/MyDrive/Colab Projects/product-pair-matching/data/raw' '/content/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-bgKRnN3LKE"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import layers, Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9eVw2v63SR3"
      },
      "source": [
        "class cfg:\n",
        "    DATA_PATH = '/content/raw'\n",
        "    CSV_TRAIN_PATH = os.path.sep.join([DATA_PATH, 'new_training_set.csv'])\n",
        "    CSV_TEST_PATH = os.path.sep.join([DATA_PATH, 'new_test_set.csv'])\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 25\n",
        "    BASE_OUTPUT = '/content/outputs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0WZmn3O3dfS"
      },
      "source": [
        "train_df = pd.read_csv(cfg.CSV_TRAIN_PATH, index_col=0)\n",
        "train, hold = train_test_split(train_df, test_size=0.3, random_state=42)\n",
        "valid, test = train_test_split(hold, test_size=0.3, random_state=42)\n",
        "\n",
        "DEBUG = False\n",
        "if DEBUG:\n",
        "    cfg.EPOCHS = 2\n",
        "    train_data = train.head(100).to_numpy()\n",
        "    valid_data = valid.head(20).to_numpy()\n",
        "else:\n",
        "    train_data = train.to_numpy()\n",
        "    valid_data = valid.to_numpy()\n",
        "    test_data = test.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXxjAgK24JwY"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_KDDIjw3ir7"
      },
      "source": [
        "default_stop_words = [\n",
        "    'atau', 'dan', 'and', 'murah', 'grosir',\n",
        "    'untuk', 'termurah', 'cod', 'terlaris', 'bisacod', 'terpopuler',\n",
        "    'bisa', 'terbaru', 'tempat', 'populer', 'di', 'sale', 'bayar', 'flash',\n",
        "    'promo', 'seler', 'in', 'salee', 'diskon', 'gila', 'starseller', 'seller'\n",
        "]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    s = str(text).lower()\n",
        "    s = ' '.join([word for word in s.split() if word not in default_stop_words])\n",
        "    s = re.sub('&', ' and ', s)\n",
        "    s = re.sub('/', 'atau', s, count=1)\n",
        "    s = re.sub(r\"[^a-zA-Z0-9]+\", ' ', s)\n",
        "    s = re.sub(' s ', 's ', s)\n",
        "    s = re.sub(r\"([0-9]+(\\.[0-9]+)?)\", r\" \\1 \", s).strip()\n",
        "    return s\n",
        "\n",
        "def preprocess_data(data):\n",
        "    data[:,0] = np.array(list(map(preprocess_text, data[:,0])))\n",
        "    data[:,2] = np.array(list(map(preprocess_text, data[:,2])))\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irx4PrHX3n9p",
        "outputId": "c64bbb94-d194-4deb-c2d9-31d4f98fe3b5"
      },
      "source": [
        "%%time\n",
        "train_data = preprocess_data(train_data)\n",
        "valid_data = preprocess_data(valid_data)\n",
        "test_data = preprocess_data(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 296 ms, sys: 3.03 ms, total: 299 ms\n",
            "Wall time: 300 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V_rj0tc3tIn"
      },
      "source": [
        "def map_func(title_1, title_2, label):\n",
        "    return {\n",
        "        'title_1': title_1,\n",
        "        'title_2': title_2\n",
        "    }, label\n",
        "\n",
        "def create_dataset(data, batch_size=cfg.BATCH_SIZE):\n",
        "    title_1 = data[:,0]\n",
        "    title_2 = data[:,2]\n",
        "    label = np.array(data[:,4], dtype='int')\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(\n",
        "        (title_1, title_2, label)\n",
        "    )\n",
        "\n",
        "    ds = ds.map(map_func, num_parallel_calls=AUTOTUNE)\n",
        "    # ds = ds.cache('/content/outputs/cache/dump.tfcache') \n",
        "    ds = ds.shuffle(buffer_size=1024)\n",
        "    ds = ds.repeat()\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "    return ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez48O52D38y6",
        "outputId": "d15b62b4-dee9-47bf-fe3b-01e81d1ba3d1"
      },
      "source": [
        "%%time\n",
        "train_ds = create_dataset(train_data)\n",
        "valid_ds = create_dataset(valid_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 485 ms, sys: 318 ms, total: 803 ms\n",
            "Wall time: 819 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsRWrsAg3-xS",
        "outputId": "79630eaf-0892-48a2-cbf3-a14f27942708"
      },
      "source": [
        "for _data, _label in valid_ds.unbatch().take(1):\n",
        "    print('Label {}'.format(_label.numpy()))\n",
        "    print(_data['title_1'].numpy())\n",
        "    print(_data['title_2'].numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label 1\n",
            "b'minyak ikan ecer  10  caps repack atau vitamin kucing anjing vitamin bulu kucing anjing tung hai'\n",
            "b'minyak ikan ecer  100  caps repack atau vitamin anjing kucing'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx2V6Pjl4F3-"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al8otdB_4OwE"
      },
      "source": [
        "def create_text_extractor(vectorize_layer):\n",
        "    inputs = layers.Input(shape=(1,), dtype='string', name='input_text')\n",
        "    x = vectorize_layer(inputs)\n",
        "    x = layers.Embedding(len(vectorize_layer.get_vocabulary()),\n",
        "                         output_dim=64,\n",
        "                         mask_zero=True)(x)\n",
        "    x = layers.GRU(64)(x)\n",
        "    outputs = layers.Dense(48, activation='relu', name='output_text')(x)\n",
        "    model = Model(inputs, outputs, name='text_extractor')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suOWme8O4R9-"
      },
      "source": [
        "def euclidean_distance(vectors):\n",
        "\t(feats_a, feats_b) = vectors\n",
        "\tnum_squared = K.sum(K.square(feats_a - feats_b), axis=1,\n",
        "\t\tkeepdims=True)\n",
        "\treturn K.sqrt(K.maximum(num_squared, K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKA5ZSmw4TfW"
      },
      "source": [
        "def create_vectorize_layer(vocab, vocab_size=10000, sequence_length=128):\n",
        "    vectorize_layer = layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "    vectorize_layer.adapt(vocab)\n",
        "\n",
        "    # tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=None)\n",
        "    # tokenizer.fit_on_texts(np.concatenate([data[:,0], data[:,2]]))\n",
        "    # vectorize_layer.set_vocabulary(np.array(list(tokenizer.index_word.values())))\n",
        "    # vectorize_layer.get_vocabulary()\n",
        "    return vectorize_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7a1gFGJ4VhR"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "title_1 = layers.Input(shape=(1,), dtype='string', name='title_1')\n",
        "title_2 = layers.Input(shape=(1,), dtype='string', name='title_2')\n",
        "\n",
        "vectorize_layer = create_vectorize_layer(vocab=np.concatenate([train_data[:,0], train_data[:,2],\n",
        "                                                               valid_data[:,0], valid_data[:,2]]))\n",
        "text_extractor = create_text_extractor(vectorize_layer)\n",
        "feats_text_1 = text_extractor(title_1)\n",
        "feats_text_2 = text_extractor(title_2)\n",
        "\n",
        "distance = layers.Lambda(euclidean_distance)([feats_text_1, feats_text_2])\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\", dtype='float32', name='final_dense')(distance)\n",
        "model = Model(inputs=[title_1, title_2], outputs=outputs, name='siamese_networks')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNHe702C41B7"
      },
      "source": [
        "metrics_f1 = tfa.metrics.F1Score(num_classes=1,threshold=0.5, name='f1')\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', metrics_f1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyvq8sRw45H7"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJvJbO0243VX",
        "outputId": "2f6bb918-a861-4371-fd13-557148ea56eb"
      },
      "source": [
        "steps_per_epoch = len(train_data)//cfg.BATCH_SIZE\n",
        "validation_steps = len(valid_data)//cfg.BATCH_SIZE\n",
        "\n",
        "model.fit(train_ds, \n",
        "          validation_data=valid_ds,\n",
        "          epochs=cfg.EPOCHS,\n",
        "          steps_per_epoch=steps_per_epoch,\n",
        "          validation_steps=validation_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "222/222 [==============================] - 15s 31ms/step - loss: 0.6910 - accuracy: 0.5614 - f1: 0.6398 - val_loss: 0.6639 - val_accuracy: 0.6624 - val_f1: 0.7574\n",
            "Epoch 2/25\n",
            "222/222 [==============================] - 5s 21ms/step - loss: 0.6431 - accuracy: 0.6822 - f1: 0.7540 - val_loss: 0.6398 - val_accuracy: 0.6799 - val_f1: 0.7598\n",
            "Epoch 3/25\n",
            "222/222 [==============================] - 5s 21ms/step - loss: 0.5725 - accuracy: 0.7402 - f1: 0.7970 - val_loss: 0.6298 - val_accuracy: 0.6832 - val_f1: 0.7667\n",
            "Epoch 4/25\n",
            "222/222 [==============================] - 5s 22ms/step - loss: 0.5258 - accuracy: 0.7699 - f1: 0.8201 - val_loss: 0.6734 - val_accuracy: 0.6870 - val_f1: 0.7615\n",
            "Epoch 5/25\n",
            "222/222 [==============================] - 5s 21ms/step - loss: 0.4828 - accuracy: 0.7910 - f1: 0.8349 - val_loss: 0.6689 - val_accuracy: 0.6870 - val_f1: 0.7604\n",
            "Epoch 6/25\n",
            "222/222 [==============================] - 5s 21ms/step - loss: 0.4520 - accuracy: 0.8093 - f1: 0.8491 - val_loss: 0.6755 - val_accuracy: 0.6970 - val_f1: 0.7701\n",
            "Epoch 7/25\n",
            "222/222 [==============================] - 4s 20ms/step - loss: 0.4343 - accuracy: 0.8168 - f1: 0.8533 - val_loss: 0.6806 - val_accuracy: 0.6937 - val_f1: 0.7606\n",
            "Epoch 8/25\n",
            "222/222 [==============================] - 5s 20ms/step - loss: 0.4076 - accuracy: 0.8326 - f1: 0.8633 - val_loss: 0.7020 - val_accuracy: 0.6979 - val_f1: 0.7619\n",
            "Epoch 9/25\n",
            "222/222 [==============================] - 5s 21ms/step - loss: 0.3978 - accuracy: 0.8413 - f1: 0.8740 - val_loss: 0.6886 - val_accuracy: 0.7050 - val_f1: 0.7692\n",
            "Epoch 10/25\n",
            "222/222 [==============================] - 4s 20ms/step - loss: 0.3742 - accuracy: 0.8533 - f1: 0.8826 - val_loss: 0.6907 - val_accuracy: 0.7003 - val_f1: 0.7660\n",
            "Epoch 11/25\n",
            "222/222 [==============================] - 5s 21ms/step - loss: 0.3632 - accuracy: 0.8578 - f1: 0.8842 - val_loss: 0.7405 - val_accuracy: 0.7022 - val_f1: 0.7622\n",
            "Epoch 12/25\n",
            "222/222 [==============================] - 5s 21ms/step - loss: 0.3489 - accuracy: 0.8702 - f1: 0.8951 - val_loss: 0.7451 - val_accuracy: 0.7107 - val_f1: 0.7681\n",
            "Epoch 13/25\n",
            "222/222 [==============================] - 5s 21ms/step - loss: 0.3461 - accuracy: 0.8675 - f1: 0.8926 - val_loss: 0.7342 - val_accuracy: 0.7064 - val_f1: 0.7678\n",
            "Epoch 14/25\n",
            "222/222 [==============================] - 5s 21ms/step - loss: 0.3438 - accuracy: 0.8714 - f1: 0.8961 - val_loss: 0.7521 - val_accuracy: 0.7079 - val_f1: 0.7667\n",
            "Epoch 15/25\n",
            "222/222 [==============================] - 5s 22ms/step - loss: 0.3317 - accuracy: 0.8750 - f1: 0.8978 - val_loss: 0.7399 - val_accuracy: 0.7064 - val_f1: 0.7690\n",
            "Epoch 16/25\n",
            "222/222 [==============================] - 5s 22ms/step - loss: 0.3187 - accuracy: 0.8834 - f1: 0.9054 - val_loss: 0.7652 - val_accuracy: 0.7031 - val_f1: 0.7624\n",
            "Epoch 17/25\n",
            "222/222 [==============================] - 5s 21ms/step - loss: 0.3212 - accuracy: 0.8762 - f1: 0.8979 - val_loss: 0.7882 - val_accuracy: 0.7036 - val_f1: 0.7603\n",
            "Epoch 18/25\n",
            "222/222 [==============================] - 5s 21ms/step - loss: 0.3236 - accuracy: 0.8783 - f1: 0.9007 - val_loss: 0.7951 - val_accuracy: 0.7074 - val_f1: 0.7648\n",
            "Epoch 19/25\n",
            "222/222 [==============================] - 5s 21ms/step - loss: 0.3049 - accuracy: 0.8890 - f1: 0.9098 - val_loss: 0.7958 - val_accuracy: 0.7060 - val_f1: 0.7634\n",
            "Epoch 20/25\n",
            "222/222 [==============================] - 5s 22ms/step - loss: 0.2945 - accuracy: 0.8946 - f1: 0.9141 - val_loss: 0.7800 - val_accuracy: 0.7069 - val_f1: 0.7665\n",
            "Epoch 21/25\n",
            "222/222 [==============================] - 5s 22ms/step - loss: 0.2981 - accuracy: 0.8927 - f1: 0.9131 - val_loss: 0.7900 - val_accuracy: 0.7069 - val_f1: 0.7633\n",
            "Epoch 22/25\n",
            "222/222 [==============================] - 5s 20ms/step - loss: 0.3000 - accuracy: 0.8906 - f1: 0.9106 - val_loss: 0.7994 - val_accuracy: 0.7045 - val_f1: 0.7633\n",
            "Epoch 23/25\n",
            "222/222 [==============================] - 5s 21ms/step - loss: 0.2839 - accuracy: 0.9002 - f1: 0.9184 - val_loss: 0.7760 - val_accuracy: 0.7064 - val_f1: 0.7659\n",
            "Epoch 24/25\n",
            "222/222 [==============================] - 5s 21ms/step - loss: 0.2971 - accuracy: 0.8923 - f1: 0.9122 - val_loss: 0.8097 - val_accuracy: 0.7017 - val_f1: 0.7606\n",
            "Epoch 25/25\n",
            "222/222 [==============================] - 5s 22ms/step - loss: 0.2885 - accuracy: 0.8947 - f1: 0.9129 - val_loss: 0.8255 - val_accuracy: 0.7045 - val_f1: 0.7609\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5d903a89e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q13gI6vZSIyR"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNj4uXv446nn",
        "outputId": "c4be1081-10e6-4040-f544-827ccf66c107"
      },
      "source": [
        "pred_data = test_data.copy()\n",
        "pred = model.predict({\n",
        "        'title_1': pred_data[:,0],\n",
        "        'title_2': pred_data[:,2]})\n",
        "\n",
        "print('Accuracy: {}'.format(accuracy_score(pred_data[:,4].astype(int), np.hstack(np.round(pred)).astype(int))))\n",
        "print('F1 Score: {}'.format(f1_score(pred_data[:,4].astype(int), np.hstack(np.round(pred)).astype(int))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7142857142857143\n",
            "F1 Score: 0.7713787085514834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsRu5ua4HlDV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}